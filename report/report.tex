\documentclass[twocolumn, 10pt, a4paper]{article}
\usepackage[a4paper, left = 0.5cm,right = 0.5cm, top = 0.5cm, bottom = 1cm, footskip = 0.5cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{arydshln}
\usepackage{pgfplots}
\usepackage[style = numeric, sorting = none]{biblatex}
 
\addbibresource{refs.bib}

\graphicspath{{./images/}}

\usetikzlibrary{shapes.geometric, arrows, calc}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=0cm, minimum height=0cm, text centered, text width=1cm, draw=black, fill=white!30]
\tikzstyle{process} = [rectangle, minimum width=0cm, minimum height=0cm, text centered, text width=4.8cm, draw=black, fill=white!30]
\tikzstyle{decision} = [diamond, aspect = 1.25, minimum width=0cm, minimum height=0cm, text centered, text width=3cm, draw=black, fill=white!30, inner sep = -1.5ex]
\tikzstyle{arrow} = [thick,->,>=stealth]

\pgfplotsset{compat = 1.17}

\author{
  % George Herbert\\
  % \texttt{cj19328@bristol.ac.uk}
}
\date{}

\title{\vspace{-2em}No Entry Sign Challenge Report\vspace{-2em}}

\begin{document}

\maketitle

\section{The Viola-Jones object detector}

\subsection{Ground truth and visualisation}

\begin{figure}[htbp]
  \centering
  \subfloat[\texttt{NoEntry1.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry1.jpg}\label{fig:face1}}
  \hfill
  \subfloat[\texttt{NoEntry2.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry2.jpg}\label{fig:face2}}
  \hfill
  \subfloat[\texttt{NoEntry4.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry4.jpg}\label{fig:face4}}
  \hfill
  \subfloat[\texttt{NoEntry5.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry5.jpg}\label{fig:face5}}
  \hfill
  \subfloat[\texttt{NoEntry7.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry7.jpg}\label{fig:face7}}
  \hfill
  \subfloat[\texttt{NoEntry11.jpg}]{\includegraphics[width = 0.24\textwidth]{images/NoEntry11.jpg}\label{fig:face11}}
  \caption{Six images with the ground truth bounding boxes in red and detected instances from \texttt{face.cpp} in green}\label{fig:face}
\end{figure}

Ground truth bounding boxes assist in determining the accuracy of a detection algorithm and help visualise how well it performs.
Figure \ref{fig:face} displays six images with ground truth bounding boxes in red and the frontal faces detected by \texttt{face.cpp} in green.
I defined the ground truth frontal faces as those whereby the majority of the face was visible, the person was facing the camera (i.e. both eyes visible), and the person was close enough for the main facial characteristics to be distinguishable.

\subsection{Intersection-over-union, true positive rate and F\textsubscript{1} score}

True positive rate (TPR) is a popular metric used to assess the performance of an object detection algorithm.
The TPR is the proportion of objects that an algorithm is attempting to detect that are actually detected, and is given by the formula:
\[
  \textrm{TPR} = \frac{\textrm{TP}}{\textrm{TP + FN}}
\]
where TP is the number of true positives, and FN is the number of false negatives.

The first practical difficulty that arises in calculating the TPR is defining a predicted bounding box as a true positive or a false positive.
I opted to define a detected bounding box as a true positive if it had an intersection-over-union (IOU) value greater than 0.5 with a ground truth bounding box, which is considered a good score \cite{iou}.
For two bounding boxes $A$ and $B$, the IOU is calculated as follows:
\[
  \textrm{IOU}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\]
where $|A \cap B|$ is the area of intersection, and $|A \cup B|$ is the area of union.
Moreover, for each ground truth bounding box, I only defined the detected bounding box with the largest IOU value as a true positive if there was more than one intersecting detected bounding box.

In any detection task, TPR can be a flawed metric to define how well a given model performs; this is because, firstly, the TPR can usually be artificially increased simply by lowering the IOU threshold, without the model performing any better in reality.
Secondly, it is possible to achieve a TPR of 100\% by detecting every possible pixel region as the object being detected, despite having many false positives.

As a result of this, the F\textsubscript{1} score is often used to measure of a model's accuracy.
F\textsubscript{1} score is the harmonic mean of recall (i.e. TPR) and precision (i.e. the proportion of predicted positives that are positive), and is calculated as follows:
\[
  \textrm{F\textsubscript{1}} = \frac{\textrm{TP}}{\textrm{TP} + \frac{1}{2}(\textrm{FP} + \textrm{FN})}
\]
where FP is the number of false positives. 

\begin{table}[htbp]
  \begin{center}
  \caption{TPRs and F\textsubscript{1} scores of the frontal face detector}\label{tab:face}
  \begin{tabular}{l | l l} 
    \hline\hline
    Image&TPR&F\textsubscript{1} Score\\
    \hline
    \texttt{NoEntry0.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry1.jpg}&1.00&0.20\\ 
    \texttt{NoEntry2.jpg}&0.25&0.18\\ 
    \texttt{NoEntry3.jpg}&Undefined&Undefined\\ 
    \texttt{NoEntry4.jpg}&1.00&0.28\\ 
    \texttt{NoEntry5.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry6.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry7.jpg}&0.50&0.22\\ 
    \texttt{NoEntry8.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry9.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry10.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry11.jpg}&0.50&0.31\\ 
    \texttt{NoEntry12.jpg}&0.00&0.00\\ 
    \texttt{NoEntry13.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry14.jpg}&Undefined&0.00\\ 
    \texttt{NoEntry15.jpg}&Undefined&0.00\\ 
    \hdashline
    All images&0.52&0.15\\ 
    \hline
  \end{tabular}
  \end{center}
\end{table} 

Table \ref{tab:face} displays the TPR and F\textsubscript{1} score that \texttt{face.cpp} achieved on each of the \texttt{NoEntry\textasteriskcentered.jpg} images.
Due to the TPR and F\textsubscript{1} score definitions, many of the values are undefined since division by zero yields an undefined result.

\clearpage

\section{Building and testing my detector}

\subsection{Training performance}

To train my own detector, I initially generated a collection of 500 artificial $30 \times 30$ images of no entry signs by applying different perspective transformations to a base image.
I then used a variant of AdaBoost to train a no entry sign detector with these 500 samples.

\begin{figure}[h]
  \pgfplotstableread{
  0 1        1
  1 1        0.0214601
  2 1        0.000129286
  }\dataset
  \begin{tikzpicture}
  \begin{axis}[
    ybar,
    ymode = log,
    log origin = infty,
    width = 0.49\textwidth,
    ymax = 1,
    enlarge x limits = 0.25,
    enlarge y limits = 0.05,
    ylabel = {Rate},
    xlabel = {Training Stage},
    xtick = data,
    xticklabels = {0, 1, 2},
  ]
  \addplot[draw = black, fill = green!50] table[x index = 0,y index = 1] \dataset;
  \addplot[draw = black, fill = red!50] table[x index = 0,y index = 2] \dataset;
  \legend{TPR, FPR}
  \end{axis}
  \end{tikzpicture}
  \caption{TPR and FPR of stages in the Viola-Jones no entry sign training process}\label{vj_training}
\end{figure}

The false positive rate (FPR) is the number of negatives incorrectly predicted as being a positive no entry sign during the training process.
Figure \ref{vj_training} displays the TPR and FPR at each stage of the training process.
The TPR remains at 100\% at each successive stage, while the FPR decreases drastically; this is because each successive stage produces a cascade of more complex classifiers.
Therefore, the early stages involve only the less complex (but faster) classifiers and have a larger FPR.
 
\subsection{Testing performance}

\begin{figure}[htbp]
  \centering
  \subfloat[\texttt{NoEntry2.jpg}]{\includegraphics[width = 0.3115\textwidth]{images/task_2_2.jpg}\label{fig:task_2_2}}
  \hfill
  \subfloat[\texttt{NoEntry8.jpg}]{\includegraphics[width = 0.1695\textwidth]{images/task_2_8.jpg}\label{fig:task_2_8}}
  \hfill
  \subfloat[\texttt{NoEntry3.jpg}]{\includegraphics[width = 0.49\textwidth]{images/task_2_3.jpg}\label{fig:task_2_3}}
  \caption{Three images with the ground truth bounding boxes in red and detected instances from the no entry sign detector in green}\label{fig:task_2}
\end{figure}

Figure \ref{fig:task_2} contains three images with ground truth bounding boxes in red and detected bounding boxes from the Viola-Jones implementation in green.
I defined the ground truth no entry signs as those whereby more than 50\% of the sign was visible.
The implementation works relatively well, achieving a TPR of 0.44 and an F\textsubscript{1} score of 0.48, as shown in Table \ref{tab:vj}.
However, the implementation has its shortcomings.
When tested, the Viola-Jones implementation detected a large number of false positives.
Upon inspection, this occurs in regions containing a slightly lighter strip horizontally between two darker strips.
For example, Figure \ref{fig:task_2_3} contains a left turn sign incorrectly detected in this manner.

The TPR on the test data is significantly lower than on the training data.
One reason for this is likely due to the differing appearances of no entry signs in the test data.
The Viola-Jones implementation was trained with a collection of artificial positives.
This approach works best for completely rigid objects, which, due to the slightly differing appearances of the no entry signs between countries, the positives in our test images are not \cite{training}.
Additionally, we are only testing our implementation on 16 images; we would ideally test our implementation on a more extensive test set to get a more accurate idea of the TPR and F\textsubscript{1} score

\begin{table}[htbp]
  \begin{center}
  \caption{TPRs and F\textsubscript{1} scores of Viola-Jones no entry sign detector}\label{tab:vj}
  \begin{tabular}{l | l l} 
    \hline\hline
    Image&TPR&F\textsubscript{1} Score\\
    \hline
    \texttt{NoEntry0.jpg}&1.00&0.67\\ 
    \texttt{NoEntry1.jpg}&1.00&0.40\\ 
    \texttt{NoEntry2.jpg}&1.00&0.40\\ 
    \texttt{NoEntry3.jpg}&1.00&0.80\\ 
    \texttt{NoEntry4.jpg}&0.50&0.67\\ 
    \texttt{NoEntry5.jpg}&0.40&0.47\\ 
    \texttt{NoEntry6.jpg}&0.00&0.00\\ 
    \texttt{NoEntry7.jpg}&0.00&0.00\\
    \texttt{NoEntry8.jpg}&0.57&0.72\\ 
    \texttt{NoEntry9.jpg}&0.00&0.00\\ 
    \texttt{NoEntry10.jpg}&0.67&0.67\\ 
    \texttt{NoEntry11.jpg}&0.50&0.25\\ 
    \texttt{NoEntry12.jpg}&0.25&0.40\\ 
    \texttt{NoEntry13.jpg}&0.00&0.00\\ 
    \texttt{NoEntry14.jpg}&1.00&1.00\\ 
    \texttt{NoEntry15.jpg}&0.50&0.67\\ 
    \hdashline
    All images&0.44&0.48\\ 
    \hline
  \end{tabular}
  \end{center}
\end{table} 

\clearpage

\section{Integration with shape detectors}

\subsection{Hough Details}

\begin{figure}[H]
  \vspace{-4.5em}
  \centering
  \subfloat[\texttt{NoEntry6.jpg} with detected bounding boxes]{\includegraphics[width = 0.24\textwidth]{images/task_3_6.jpg}}
  \hfill
  \subfloat[\texttt{NoEntry2.jpg} with detected bounding boxes]{\includegraphics[width = 0.24\textwidth]{images/task_3_2.jpg}}
  \hfill
  \subfloat[\texttt{NoEntry6.jpg} gradient magnitude threshold]{\includegraphics[width = 0.24\textwidth]{images/6_gradient_magnitude_threshold.jpg}}
  \hfill
  \subfloat[\texttt{NoEntry2.jpg} gradient magnitude threshold]{\includegraphics[width = 0.24\textwidth]{images/2_gradient_magnitude_threshold.jpg}}
  \hfill
  \subfloat[\texttt{NoEntry6.jpg} 2D Hough space]{\includegraphics[width = 0.24\textwidth]{images/6_hough_space.jpg}}
  \hfill
  \subfloat[\texttt{NoEntry2.jpg} 2D Hough space]{\includegraphics[width = 0.24\textwidth]{images/2_hough_space.jpg}}
  \caption{Two no entry sign images at various stages of the integrated implementation}\label{fig:hough_details}
\end{figure}

\subsection{Evaluation}

Table \ref{tab:shape} displays the TPR and F\textsubscript{1} score of the integrated implementation and the differences compared with the Viola-Jones implementation.
The key merits and shortcomings include:
\begin{itemize}
\itemsep 0em 
\item A significantly larger F\textsubscript{1} score, primarily as a result of increasing the precision; achieved by only defining an object detected by the Viola-Jones detector as positive if it has an IOU value with a  bounding box from the circle Hough Transform greater than 50\%
\item A slight improvement to the TPR score, by reducing the minimum  number of neighbours allowing for more true positives to be identified (the extra false positives were `filtered out' by utilising the circle Hough Transform)
\item If a no entry sign is positively detected by one of the detectors, but not the other, it is not positively detected by the integrated implementation
\item A significantly slower implementation, since the circle Hough Transform takes longer to calculate
\end{itemize}

\subsection{Detection pipeline}

Figure \ref{fig:flow} outlines the way I combined evidence in my algorithm.
The rationale behind this was as follows:
\begin{itemize}
\itemsep 0em 
\item The Viola-Jones detector detected 44\% of no entry signs but had a low level of precision
\item Upon inspection of the detected bounding boxes, it became clear the Viola-Jones detector appeared to be detecting regions with a light bar horizontally between two dark bars
\item Since no entry signs are circles, we could choose to detect the circles from the circle Hough Transform that have an IOU with a Viola-Jones bounding box of greater than 50\% (i.e. they are likely detecting the same object)
\end{itemize}

It is important to note that if multiple circles had an IOU greater than 50\% with a Viola-Jones detected bounding box, only the circle with the largest IOU was positively identified to avoid duplicate identifications of the same no entry sign.

\begin{table}[H]
  \begin{center}
  \caption{TPRs and F\textsubscript{1} scores of the integrated implementation, and the differences compared to the Viola-Jones implementation}\label{tab:shape}
  \begin{tabular}{l | l l | l l} 
    \hline\hline
    &\multicolumn{2}{| c |}{Result}&\multicolumn{2}{| c}{Difference}\\
    Image&TPR&F\textsubscript{1} Score&TPR&F\textsubscript{1} Score\\
    \hline
    \texttt{NoEntry0.jpg}&1.00&1.00&$\pm0.00$&+0.33\\
    \texttt{NoEntry1.jpg}&1.00&1.00&$\pm0.00$&+0.60\\
    \texttt{NoEntry2.jpg}&1.00&1.00&$\pm0.00$&+0.60\\
    \texttt{NoEntry3.jpg}&1.00&1.00&$\pm0.00$&+0.20\\
    \texttt{NoEntry4.jpg}&1.00&1.00&+0.50&+0.33\\
    \texttt{NoEntry5.jpg}&0.20&0.31&$-0.20$&$-0.16$\\
    \texttt{NoEntry6.jpg}&0.00&0.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry7.jpg}&0.00&0.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry8.jpg}&0.43&0.60&$-0.14$&$-0.13$\\
    \texttt{NoEntry9.jpg}&1.00&1.00&+1.00&+1.00\\
    \texttt{NoEntry10.jpg}&1.00&1.00&+0.33&+0.33\\
    \texttt{NoEntry11.jpg}&0.50&0.67&$\pm0.00$&+0.42\\
    \texttt{NoEntry12.jpg}&0.38&0.55&+0.13&+0.16\\
    \texttt{NoEntry13.jpg}&0.00&0.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry14.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry15.jpg}&1.00&1.00&+0.50&+0.33\\
    \hdashline
    All images&0.50&0.66&+0.04&+0.17\\
    \hline
  \end{tabular}
  \end{center}
\end{table} 

\begin{figure}[H]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tikzpicture}[node distance = 1.25cm]
  \node (start) [startstop] {Start};
  \node (hough) [process, below of = start] {Create set of bounding boxes of circles from the circle Hough Transform};
  \node (vj) [process, right of = hough, xshift = 11.75em] {Create set of bounding boxes of no entry signs from Viola-Jones detector};
  \node (empty) [process, below of = vj, yshift = -0.6em] {Create empty list for positive no entry signs};
  \node (cartesian) [process, left of = empty, xshift = -11.75em] {Create cartesian product of bounding boxes from both sets};
  \node (for) [process, below of = cartesian, yshift = -0.8em] {For each combination of bounding boxes in the cartesian product};
  \node (iou) [decision, below of = for, yshift = -5em] {IOU of\\combination greater\\than 50\%?};
  \node (list) [process, below of = iou, yshift = -3.3em] {Add circle bounding box to list of positive no entry signs};
  \node (done) [decision, right of = iou, xshift = 10em] {Have the\\IOUs of all\\combinations been\\calculated?};
  \node (stop) [startstop, below of = done, yshift = -2.65em] {Stop};
  \draw [arrow] (start) -- (hough);
  \draw [arrow] (hough) -- (vj);
  \draw [arrow] (vj) -- (empty);
  \draw [arrow] (empty) -- (cartesian);
  \draw [arrow] (cartesian) -- (for);
  \draw [arrow] (for) -- node[anchor = east, text width = 2cm] {\centering First\\combination} (iou);
  \draw [arrow] (iou) -- node[anchor = east] {Yes} (list);
  \draw [arrow] (iou) -- node[anchor = north] {No} (done);
  \draw [arrow] (list.east) -| (done.west);
  \draw let \p1 = (done.north) in [thick,-] (done.north) -- node[anchor = west] {No} (\x1, -5.25);
  \draw let \p1 = (done.north), \p2 = (iou.north) in [arrow] (\x1, -5.25) -- node[anchor = north] {Next combination} (\x2, -5.25);
  \draw [arrow] (done) -- node[anchor = west] {Yes} (stop);
\end{tikzpicture}
}
\caption{Flow chart detailing my algorithm that integrates Viola-Jones with the circle Hough Transform}
\label{fig:flow}
\end{figure}

\clearpage

\section{Improving my detector}

\subsection{Idea}

My improved approach puts every circle detected by the circle Hough Transform but not by the Viola-Jones detector through an extra processing stage.
The extra stage of processing begins by converting the pixels' colour space within the circle from RGB to CIELAB.
$K$-means is then performed with $k = 2$ over the green-red axis $a*$ and blue-yellow axis $b*$.
The Euclidean distance is then calculated from the cluster centres to pure red and pure white to identify the two main colours in the circle.
If the distances are less than a threshold (i.e. the two main colours are red and white), the line Hough Transform is conducted on the circle to identify horizontal parallel lines.
If horizontal parallel lines are detected, the circle is positively identified as a no entry sign.
The rationale behind this was as follows:
\begin{itemize}
  \item The pixels are transformed from RGB to CIELAB so the $L*$ axis (i.e. the perceptual brightness axis) can be ignored when doing $k$-means, so that no entry signs under a variety of lighting conditions can be detected.
  \item Since CIELAB is perceptually uniform, by calculating the Euclidean distance, the algorithm can confirm whether the primary colours in the circle are red and white
  \item By performing the line Hough Transform, we can filter out all circles that contain red and white but are not no entry signs
\end{itemize}

\subsection{Visualise}

\begin{figure}[H]
  \vspace{-4.5em}
  \centering
  \subfloat[\texttt{NoEntry6.jpg} with detected bounding boxes]{\includegraphics[width = 0.24\textwidth]{images/task_4_6.jpg}\label{fig:improved_image_6}}
  \hfill
  \subfloat[\texttt{NoEntry13.jpg} with detected bounding boxes]{\includegraphics[width = 0.24\textwidth]{images/task_4_13.jpg}\label{fig:improved_image_13}}
  \hfill
  \caption{Two no entry sign images with detected bounding boxes from my improved detector}\label{fig:improved_images}
\end{figure}

Figure \ref{fig:improved_images} exhibits the merit of my improved approach.
The previous detector did not identify any of the detected bounding boxes.

The two bounding boxes in Figure \ref{fig:improved_image_6} very clearly contain no entry signs.
They are detected by the circle Hough Transform but not the Viola-Jones detector.
Therefore, my improved algorithm puts them through the extra stage of processing.
A merit of using $k$-means is evident here: it can identify the two main colours in the circles by finding the values of the cluster centres.
A merit of using CIELAB is also apparent: since CIELAB is perceptually uniform, my improved detector can identify the two main colours as red and white by calculating the Euclidean distance.
A merit of using the line Hough Transform is that my algorithm can confirm that the circles are no entry signs by detecting two horizontal parallel lines.
Therefore, my algorithm can eliminate false positives.
Without it, my algorithm would identify any circle containing red and white as a no entry sign.

The no entry sign in Figure \ref{fig:improved_image_13} is clearly a no entry sign, but potentially due to the inadequate lighting, it is not identified by the Viola-Jones detector.
However, it is detected by the circle Hough Transform and is therefore put through the extra processing stage.
Another merit of converting the pixels from RGB to CIELAB is evident here: by focusing purely on the $a*$ and $b*$ axes, no entry sign can be identified, despite the poor lighting conditions.

\subsection{Evaluate}

\begin{table}[htbp]
  \begin{center}
  \caption{TPRs and F\textsubscript{1} scores of the improved implementation and the differences compared to the previous implementation}\label{tab:final}
  \begin{tabular}{l | l l | l l} 
    \hline\hline
    &\multicolumn{2}{| c |}{Result}&\multicolumn{2}{| c}{Difference}\\
    Image&TPR&F\textsubscript{1} Score&TPR&F\textsubscript{1} Score\\
    \hline
    \texttt{NoEntry0.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry1.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry2.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry3.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry4.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry5.jpg}&0.20&0.31&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry6.jpg}&0.50&0.67&+0.50&+0.67\\
    \texttt{NoEntry7.jpg}&0.00&0.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry8.jpg}&0.43&0.60&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry9.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry10.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry11.jpg}&1.00&1.00&+0.50&+0.33\\
    \texttt{NoEntry12.jpg}&0.38&0.55&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry13.jpg}&1.00&1.00&+1.00&+1.00\\
    \texttt{NoEntry14.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \texttt{NoEntry15.jpg}&1.00&1.00&$\pm0.00$&$\pm0.00$\\
    \hdashline
    All images&0.58&0.73&+0.08&+0.07\\
    \hline
  \end{tabular}
  \end{center}
\end{table}

Table \ref{tab:final} contains the TPR and F\textsubscript{1} scores of the improved detector.
By putting detected circles that were not detected by the Viola-Jones detector through an extra stage of processing, the improved implementation detected 8\% more no entry signs and achieved a 7\% higher F\textsubscript{1} score.
The key merits and shortcomings of the improved implementation include:
\begin{itemize}
  \item No entry signs that are facing the camera (i.e. circles) are identified almost always by the circle Hough Transform, and by putting them through the extra stage of processing, they are almost always correctly identified by looking for key characteristics
  \item No entry signs that are not detected by the circle Hough Transform cannot be detected no matter what; this includes no entry signs that are at an angle (and are therefore an ellipse) and no entry signs with a significant amount of occlusion
\end{itemize}

\subsection{Further Extensions}

There are several ways I could likely improve the TPR of my detector as an extension.
Firstly, I could use the elliptical Hough Transform to detect entry signs not directly facing the camera.
However, I opted not to do this since the elliptical Hough transform requires a five-dimensional space and is significantly more computationally expensive.
Secondly, I could train the Viola-Jones detector on real no entry signs rather than artificial positives.
Ultimately, I did not do this as it was not ƒeasible to collect and process the positive samples subject to the time constraint.

\clearpage

\onecolumn{\printbibliography}
    

\end{document}